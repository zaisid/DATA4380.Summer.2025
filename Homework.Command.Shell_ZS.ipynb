{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31e80e9",
   "metadata": {},
   "source": [
    "# Unix Shell\n",
    "\n",
    "There is a lot that can be done on the Unix shell command prompt. For homework, we will do some useful manipulations of CSV files.\n",
    "\n",
    "There is plenty of material online that will help you figure out how to do various tasks on the command line. Some example resources I found by googling:\n",
    "\n",
    "* Paths and Wildcards: https://www.warp.dev/terminus/linux-wildcards\n",
    "* General introduction to shell: https://github-pages.ucl.ac.uk/RCPSTrainingMaterials/HPCandHTCusingLegion/2_intro_to_shell.html\n",
    "* Manual pages: https://www.geeksforgeeks.org/linux-man-page-entries-different-types/?ref=ml_lbp\n",
    "* Chaining commands: https://www.geeksforgeeks.org/chaining-commands-in-linux/?ref=ml_lbp\n",
    "* Piping: https://www.geeksforgeeks.org/piping-in-unix-or-linux/\n",
    "* Using sed: https://www.geeksforgeeks.org/sed-command-linux-set-2/?ref=ml_lbp\n",
    "* Various Unix commands: https://www.geeksforgeeks.org/linux-commands/?ref=lbp\n",
    "* Cheat sheets:\n",
    "    * https://www.stationx.net/unix-commands-cheat-sheet/\n",
    "    * https://cheatography.com/davechild/cheat-sheets/linux-command-line/\n",
    "    * https://www.theknowledgeacademy.com/blog/unix-commands-cheat-sheet/\n",
    "    \n",
    "These aren't necessarily the best resources. Feel free to search for better ones. Also, don't forget that Unix has built-in manual pages for all of its commands. Just type `man <command>` at the command prompt. Use the space-bar to scroll through the documentation and \"q\" to exit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd64929",
   "metadata": {},
   "source": [
    "## Homework (Due Friday 6/13)\n",
    "\n",
    "### Setup \n",
    "\n",
    "1. Make sure you have setup:\n",
    "    * Laptop setup with python, jupyter, and usual Data Science stack install via pip.\n",
    "    * Note if you are using Windows, you must setup via WSL.\n",
    "        * Must know how to read files in Windows disks from WSL Ubuntu VM?\n",
    "        * Must know how to read files in your WSL Ubuntu VM from Windows?\n",
    "\n",
    "2. Make sure you are setup to use GitHub on the command line in you Linux / MacOS environment:\n",
    "    * Make sure GitHub is properly setup\n",
    "        * Authentication\n",
    "        * Demonstrate you can push from the command prompt.\n",
    "    * Create a new repository for you work. Name it appropriately. Make it public. \n",
    "    * Organize it. e.g. You will do various projects. e.g. Sub-directories for each project.\n",
    "\n",
    "3. Install Kaggle API:\n",
    "    * Install [kaggle API](https://www.kaggle.com/docs/api).\n",
    "        * Setup your PATH environment variable.\n",
    "        * Be able to edit text files\n",
    "\n",
    "4. Create a directory in your Linux / MacOS filesystem where you will store the files. We will be using CSV files from the Kaggle challenges and datasets listed here. Download and unzip all of the datasets **using the Kaggle API**.\n",
    "    * https://www.kaggle.com/competitions/diabetes-prediction-with-nn\n",
    "    * https://www.kaggle.com/datasets/rishidamarla/heart-disease-prediction\n",
    "    * https://www.kaggle.com/competitions/used-car-price-prediction-competition\n",
    "    * https://www.kaggle.com/datasets/yasserh/housing-prices-dataset\n",
    "    * https://www.kaggle.com/datasets/simtoor/mall-customers\n",
    "    * https://www.kaggle.com/competitions/business-research-methods-world-university-rankings\n",
    "    * https://www.kaggle.com/datasets/m5anas/cancer-patient-data-sets\n",
    "        \n",
    "5. Setup for homework submission.\n",
    "    * Create a homework directory in your GitHub Repo where you will submit your solutions. \n",
    "    * Create a symbolic link from this directory to where you stored the downloaded datasets in step 4 below.\n",
    "    * **Do not commit any datafiles into GitHub.**\n",
    "        \n",
    "### Exercises        \n",
    "\n",
    "Perform all of these tasks on the Unix command prompt. Some may require several commands. Many will require chaining commands together. Once you figure out how to perform the task, copy paste the command(s) into a notebook that you will submit.\n",
    "\n",
    "\n",
    "#### 1. Organize your dataset directory. Make a new directory for the original zip files, and move the files there. In case you accidentally mess up one of the CSV files, you'll be able unzip the data again. \n",
    "\n",
    "Hint: use `mkdir` and `mv` commands with appropriate wildcards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacef436-710e-4fec-a0dc-aaf56f8e4d6c",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "    mkdir given_zips\n",
    "    mv *.zip given_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa2f596-7420-4738-9846-47d55528bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart_Disease_Prediction.csv\n",
      "Housing.csv\n",
      "Mall Customers.xlsx\n",
      "cancer patient data sets.csv\n",
      "car_web_scraped_dataset.csv\n",
      "diabetes_prediction_dataset.csv\n",
      "\u001b[34mgiven_zips\u001b[m\u001b[m\n",
      "world all university rank and rank score.csv\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e68ee17e-facf-4cc8-a708-73444d791e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast-cancer.zip\n",
      "cancer-patients-and-air-pollution-a-new-link.zip\n",
      "customer-segmentation-tutorial-in-python.zip\n",
      "heart-disease-prediction.zip\n",
      "housing-price-prediction.zip\n",
      "starbucks.zip\n",
      "titanic-dataset.zip\n",
      "weather-prediction.zip\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets/given_zips && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d468bf-b2b3-4f52-904a-a6639bfa9651",
   "metadata": {},
   "source": [
    "#### 2. The \"diabetes_prediction_dataset.csv\" file has a lot of entries. Create 3 new CSV files, each with about 1/3 of the data.\n",
    "\n",
    "Hints: \n",
    "* Use `head` to get first line.  \n",
    "* First create 3 files with just the first line by redirecting output of `head` into a file using `>`.\n",
    "* Use `wc` to count the number of entries\n",
    "* Chain/pipe `head` and `tail` to select specific lines, redirecting output to append to the 3 files you created using `>>`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e7dd385-430f-4c5b-9823-42e3862f4696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100001  142264 3810356 diabetes_prediction_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets && wc diabetes_prediction_dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b8853-8d40-48c3-8969-acd7d12a3baf",
   "metadata": {},
   "source": [
    "    100001 lines, subtract one for headers\n",
    "**100,000 lines total**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e40e4571-4636-467a-9513-02bd2888486e",
   "metadata": {},
   "source": [
    "    head -n 1 diabetes_prediction_dataset*.csv > diabetes_1.csv\n",
    "    head -n 1 diabetes_prediction_dataset*.csv > diabetes_2.csv\n",
    "    head -n 1 diabetes_prediction_dataset*.csv > diabetes_3.csv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bd4fb2c-2af9-4474-a5f2-5529203270d5",
   "metadata": {},
   "source": [
    "    head -n 33333 diabetes_prediction_dataset.csv >> diabetes_1.csv\n",
    "    head -n 66667 diabetes_prediction_dataset.csv | tail -n 33333 >> diabetes_2.csv\n",
    "    tail -n 33334 diabetes_prediction_dataset.csv >> diabetes_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "73806b2b-31a5-4d10-a5ca-069dad847e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## ^^ Help with formatting \"middle\" csv pipeline and debugging (original code was creating a csv with 100000+ rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a46f1-d03a-4c68-beda-f1dff983298e",
   "metadata": {},
   "source": [
    "#### 3. Create 2 new CSV files from `Heart_Disease_Prediction.csv`, one containing rows with \"Presence\" label and another with \"Absence\" label. Make sure that the first line of each file contains the field names. \n",
    "\n",
    "Hints: \n",
    "* Use `head` to get first line.  \n",
    "* First create 2 files with just the first line by redirecting output of `head` into a file using `>`.\n",
    "* Use `grep` to select lines that contain \"Absence\" or \"Presence\" and append the output to the appropriate file created in the previous step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cd2929b-fa50-434c-888d-f705f15836f0",
   "metadata": {},
   "source": [
    "    head -n 1 Heart_Disease_Prediction.csv > heart_disease_presence.csv\n",
    "    head -n 1 Heart_Disease_Prediction.csv > heart_disease_absence.csv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "011ec1f5-864e-4d95-901a-c8eecd04d011",
   "metadata": {},
   "source": [
    "    grep Presence Heart_Disease_Prediction.csv >> heart_disease_presence.csv\n",
    "    grep Absence Heart_Disease_Prediction.csv >> heart_disease_absence.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18634e4f-41f1-4df0-a625-6f52f17dbf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age,Sex,Chest pain type,BP,Cholesterol,FBS over 120,EKG results,Max HR,Exercise angina,ST depression,Slope of ST,Number of vessels fluro,Thallium,Heart Disease\n",
      "70,1,4,130,322,0,2,109,0,2.4,2,3,3,Presence\n",
      "57,1,2,124,261,0,0,141,0,0.3,1,0,7,Presence\n",
      "56,1,3,130,256,1,2,142,1,0.6,2,1,6,Presence\n",
      "59,1,4,110,239,0,2,142,1,1.2,2,1,7,Presence\n",
      "60,1,4,140,293,0,2,170,0,1.2,2,2,7,Presence\n",
      "63,0,4,150,407,0,2,154,0,4,2,3,7,Presence\n",
      "61,1,1,134,234,0,0,145,0,2.6,2,2,3,Presence\n",
      "46,1,4,140,311,0,0,120,1,1.8,2,2,7,Presence\n",
      "53,1,4,140,203,1,2,155,1,3.1,3,0,7,Presence\n",
      " \n",
      "Age,Sex,Chest pain type,BP,Cholesterol,FBS over 120,EKG results,Max HR,Exercise angina,ST depression,Slope of ST,Number of vessels fluro,Thallium,Heart Disease\n",
      "67,0,3,115,564,0,2,160,0,1.6,2,0,7,Absence\n",
      "64,1,4,128,263,0,0,105,1,0.2,2,1,7,Absence\n",
      "74,0,2,120,269,0,2,121,1,0.2,1,1,3,Absence\n",
      "65,1,4,120,177,0,0,140,0,0.4,1,0,7,Absence\n",
      "59,1,4,135,234,0,0,161,0,0.5,2,0,7,Absence\n",
      "53,1,4,142,226,0,2,111,1,0,1,0,7,Absence\n",
      "44,1,3,140,235,0,2,180,0,0,1,0,3,Absence\n",
      "57,0,4,128,303,0,2,159,0,0,1,1,3,Absence\n",
      "71,0,4,112,149,0,0,125,0,1.6,2,0,3,Absence\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets && head heart_disease_presence.csv && echo \" \" && head heart_disease_absence.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3df701-3117-41d6-8c0b-ccb860740bfb",
   "metadata": {},
   "source": [
    "#### 4. What fraction of cars in `car_web_scraped_dataset.csv` have had no accidents?\n",
    "\n",
    "Hints:\n",
    "* Use `grep` to select the appropriate lines.\n",
    "* Pipe the output of grep into `wc` (using `|`) to count the lines.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5428683f-a506-49a2-bd71-5597bf41f932",
   "metadata": {},
   "source": [
    "    grep 'No accidents reported' car_web_scraped_dataset.csv | wc -l\n",
    "    Output: 2223"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a2ddb40-22e9-4317-85d0-b308b0756f1f",
   "metadata": {},
   "source": [
    "    wc car_web_scraped_dataset.csv\n",
    "    Output: 2841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c835f79-e960-4900-82ac-92e51272b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of cars with no accidents: 0.7824709609292503\n"
     ]
    }
   ],
   "source": [
    "print(\"Fraction of cars with no accidents:\",2223/2841)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c241b03-bcdb-4a38-a006-a1ce4421540e",
   "metadata": {},
   "source": [
    "#### 5. Make the following replacements in `Housing.csv`, output the result into a new CSV:\n",
    "\n",
    "* yes -> 1\n",
    "* no -> 0\n",
    "* unfurnished -> 0\n",
    "* furnished -> 1\n",
    "* semi-furnished -> 2\n",
    "    \n",
    "Hints:\n",
    "* Use `sed` to do the replacement.\n",
    "* Use pipes to chain multiple `sed` commands.\n",
    "* To avoid replacing \"unfurnished\" or \"semi-furnished\" when performing the \"furnished\" replacement, try replacing \",furnished\" with \",1\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1109ce16-ea87-4186-802b-1b348e941aa0",
   "metadata": {},
   "source": [
    "    sed -e 's/yes/1/g' -e 's/no/0/g' -e 's/unfurnished/0/g' -e 's/,furnished/,1/g' -e 's/semi-furnished/2/g' Housing.csv > Housing_encoded.csv   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e1f5773-8861-4f87-a01a-bf70096a6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #^^Help figuring out sed syntax from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "257dfa54-cbf1-4539-95c1-0ab30516395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price,area,bedrooms,bathrooms,stories,mainroad,guestroom,basement,hotwaterheating,airconditioning,parking,prefarea,furnishingstatus\n",
      "13300000,7420,4,2,3,1,0,0,0,1,2,1,1\n",
      "12250000,8960,4,4,4,1,0,0,0,1,3,0,1\n",
      "12250000,9960,3,2,2,1,0,1,0,0,2,1,2\n",
      "12215000,7500,4,2,2,1,0,1,0,1,3,1,1\n",
      "11410000,7420,4,1,2,1,1,1,0,1,2,0,1\n",
      "10850000,7500,3,3,1,1,0,1,0,1,2,1,2\n",
      "10150000,8580,4,3,4,1,0,0,0,1,2,1,2\n",
      "10150000,16200,5,3,2,1,0,0,0,0,0,0,0\n",
      "9870000,8100,4,1,2,1,1,1,0,1,2,1,1\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets && head Housing_encoded.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4934c6a-079d-49f6-ba60-34beada792e2",
   "metadata": {},
   "source": [
    "#### 6. Create a new CSV files from `Mall_Customers`, removing \"CustomerID\" column.\n",
    "\n",
    "Hints:\n",
    "* Use `cut` command\n",
    "* Default separator for `cut` is the space character. For CSV, you have to use option `-d ','`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3140c23d-d550-4461-a04b-b077a5d40f41",
   "metadata": {},
   "source": [
    "    cut -f 2- -d ',' Mall_Customers.csv > Mall_Customers_noID.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d5fc7d8d-1cec-4dfe-a3e5-ecb5aea23ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender,Age,Annual Income (k$),Spending Score (1-100)\n",
      "Male,19,15,39\n",
      "Male,21,15,81\n",
      "Female,20,16,6\n",
      "Female,23,16,77\n",
      "Female,31,17,40\n",
      "Female,22,17,76\n",
      "Female,35,18,6\n",
      "Female,23,18,94\n",
      "Male,64,19,3\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets && head Mall_Customers_noID.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6fb84f-e192-4ba7-b73b-0126d5bcaedb",
   "metadata": {},
   "source": [
    "#### 7. Create a new file that contains the sum of the following fields for each row:\n",
    "    * Research Quality Score\n",
    "    * Industry Score\n",
    "    * International Outlook\n",
    "    * Research Environment Score\n",
    "    \n",
    "Hints:\n",
    "* Use `cut` to select the correct columns.\n",
    "* Use `tr` to replace ',' with '+'.\n",
    "* Pipe output into `bc` to compute the sum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b6806e2-e22d-47ab-8f1e-21c2807a578f",
   "metadata": {},
   "source": [
    "    cut -d',' -f5-8 university_rank.csv | tail -n +2 | tr ',–' '+' | bc > university_sums.csv   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5086476e-bafc-46a1-9cb5-2f354d178ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #^^Help with interpreting/fixing Parse error with bc from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ce58dad-a8d3-4e22-90c2-234606d77427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378.2\n",
      "367.2\n",
      "340.5\n",
      "361.2\n",
      "353.3\n",
      "363.0\n",
      "335.9\n",
      "354.9\n",
      "329.1\n",
      "355.7\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets && head university_sums.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f901b-bed3-4b4e-8bf4-fd608d8b566e",
   "metadata": {},
   "source": [
    "#### 8. Sort the `cancer patient data sets.csv` file by age. Make sure the output is a readable CSV file.\n",
    "\n",
    "Hints:\n",
    "* Use `sort` with `-n`, `-t`, and `-k` options. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4e2f33b-6180-4e33-aad5-b11813ae4565",
   "metadata": {},
   "source": [
    "    sort -n -k 3 -t ',' cancer_patient_dataset.csv > cancer_patient_dataset_sorted.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5e81ca96-a1b4-49ef-992c-3ef20a5b7f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "!cd HW1_Datasets && cut -d ',' -f 3 cancer_patient_dataset_sorted.csv | head -n 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
